# -*- coding: utf-8 -*-
"""object recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kX0j_RRqZqhon3QL326J02kIqp5xFC4Y
"""

import tensorflow as tf
import numpy as np
!pip install opendatasets
import opendatasets as od
import os
import torch
from torchvision import datasets, models, transforms

"""import the data"""

from google.colab import drive
drive.mount('/content/drive')

od.download('https://www.kaggle.com/datasets/mohammadamireshraghi/blood-cell-cancer-all-4class/data')

"""Count the number of each set(data augmentation and normalization)

Here we need the skimage, sklearn.model_selection
"""

import random
from skimage import io, transform, color, exposure
from skimage import data, img_as_float, img_as_ubyte
from imutils import paths
from sklearn.model_selection import train_test_split

data_dir  = '/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]'
data_list = sorted(list(paths.list_images(data_dir)))

random.seed(88)
random.shuffle(data_list)

train_list, test_list = train_test_split(data_list, train_size=0.90, shuffle=True, random_state=88)

print('number of testing list -:',len(test_list))
print('number of training list-:',len(train_list))

"""Here we use the function 'path.list_images'"""

print('Number of samples in dataset:',len(list(paths.list_images("/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]"))))

print('Number of samples in each class:','\n')
print("#1 Benign ---------------:", len(list(paths.list_images("/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]/Benign"))))
print("#2 Malignant[Early PreB] :", len(list(paths.list_images("/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]/[Malignant] early Pre-B"))))
print("#3 Malignant[PreB] ------:", len(list(paths.list_images("/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]/[Malignant] Pre-B"))))
print("#4 Malignant[ProB] ------:", len(list(paths.list_images("/content/blood-cell-cancer-all-4class/Blood cell Cancer [ALL]/[Malignant] Pro-B"))))

"""resize the image(only the training data should be prepared)"""

import cv2 as cv
p=0
for img in train_list[:]:
  i=cv.imread(img)
  i=cv.resize(i,(224,224))
  label=img.split(os.path.sep)[4]
  if (label=="Benign"):
    b= ('/content/drive/MyDrive/Prepared_test/Benign/Benign'+str(p)+'.png')
  if (label=="[Malignant] Pre-B"):
    b= ('/content/drive/MyDrive/Prepared_test/Malignant Pre-B/Malignant Pre-B'+str(p)+'.png')
  if (label=="[Malignant] Pro-B"):
    b= ('/content/drive/MyDrive/Prepared_test/Malignant Pro-B/Malignant Pro-B'+str(p)+'.png')
  if (label=="[Malignant] early Pre-B"):
    b= ('/content/drive/MyDrive/Prepared_test/Malignant early Pre-B/Malignant early Pre-B'+str(p)+'.png')
  p=p+1
  cv.imwrite(b,i)

from sklearn.cluster import KMeans
 from scipy import ndimage as ndi
 from skimage import morphology
p=0

for img in train_list[:]:

    i= cv.imread(img)
    i= cv.resize(i,(224,224))
    label= img.split(os.path.sep)[4]

    if (label=="Benign"):
        b= ('/content/drive/MyDrive/Prepareddata/Benign/Benign'+str(p)+'.png')
    if (label=="[Malignant] Pre-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant Pre-B/Malignant Pre-B'+str(p)+'.png')
    if (label=="[Malignant] Pro-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant Pro-B/Malignant Pro-B'+str(p)+'.png')
    if (label=="[Malignant] early Pre-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant early Pre-B/Malignant early Pre-B'+str(p)+'.png')
    p+=1
    cv.imwrite(b,i)
    i= cv.cvtColor(i, cv.COLOR_BGR2RGB)
    i_lab = cv.cvtColor(i, cv.COLOR_RGB2LAB)        #RGB -> LAB
    l,a,b = cv.split(i_lab)
    i2 = a.reshape(a.shape[0]*a.shape[1],1)
    km= KMeans(n_clusters=7, random_state=0,n_init=10).fit(i2)  #Clustring
    p2s= km.cluster_centers_[km.labels_]
    ic= p2s.reshape(a.shape[0],a.shape[1])
    ic = ic.astype(np.uint8)
    r,t = cv.threshold(ic,141,255 ,cv.THRESH_BINARY) #Binary Thresholding
    fh = ndi.binary_fill_holes(t)                      #fill holes
    m1 = morphology.remove_small_objects(fh, 200)
    m2 = morphology.remove_small_holes(m1,250)
    m2 = m2.astype(np.uint8)
    out = cv.bitwise_and(i, i, mask=m2)
    if (label=="Benign"):
        b= ('/content/drive/MyDrive/Prepareddata/Benign/Benign'+str(p)+'.png')
    if (label=="[Malignant] Pre-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant Pre-B/Malignant Pre-B'+str(p)+'.png')
    if (label=="[Malignant] Pro-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant Pro-B/Malignant Pro-B'+str(p)+'.png')
    if (label=="[Malignant] early Pre-B"):
        b= ('/content/drive/MyDrive/Prepareddata/Malignant early Pre-B/Malignant early Pre-B'+str(p)+'.png')
    p+=1
    out= cv.cvtColor(out, cv.COLOR_RGB2BGR)
    cv.imwrite(b,out)

import pandas as pd
prepared_data_path = '/content/drive/MyDrive/Prepared_test'
prepared_data_list_filenames = (sorted(list(paths.list_images(prepared_data_path))))
random.shuffle(prepared_data_list_filenames)
prepared_data_list_labels = []

for line in prepared_data_list_filenames:
    prepared_data_list_labels.append(line.split(os.path.sep)[3])

I_series = pd.Series(prepared_data_list_filenames, name='filenames')
L_series = pd.Series(prepared_data_list_labels, name='labels')
test_df = pd.concat( [I_series, L_series], axis=1)

print('-- test Datafarame --')
print(test_df.head())
#print number of each class:
a=test_df['labels'].value_counts()
a

"""produce the train generator"""

prepared_data_path = '/content/drive/MyDrive/Prepareddata'
prepared_data_list_filenames = (sorted(list(paths.list_images(prepared_data_path))))
random.shuffle(prepared_data_list_filenames)
prepared_data_list_labels = []

for line in prepared_data_list_filenames:
    prepared_data_list_labels.append(line.split(os.path.sep)[3])

I_series = pd.Series(prepared_data_list_filenames, name='filenames')
L_series = pd.Series(prepared_data_list_labels, name='labels')
df = pd.concat( [I_series, L_series], axis=1)
SPLIT= 0.90

TRAIN_DF, VALID_DF = train_test_split(df, train_size=SPLIT, shuffle=True, random_state=88)

from tensorflow.keras.preprocessing.image import ImageDataGenerator


BATCH_SIZE= 32
IMG_SHAPE= (224, 224, 3)
IMG_SIZE= (224, 224)

gen = ImageDataGenerator(rescale=1./255,
                         vertical_flip=True,
                         horizontal_flip=True)
                         #rotation_range=10)

gen2 = ImageDataGenerator(rescale=1./255)

train_gen = gen.flow_from_dataframe(TRAIN_DF,
                                    x_col= 'filenames',
                                    y_col= 'labels',
                                    target_size= IMG_SIZE,
                                    class_mode= 'categorical',
                                    color_mode= 'rgb',
                                    shuffle= True,
                                    batch_size= BATCH_SIZE,
                                    seed=88
)

valid_gen= gen2.flow_from_dataframe(VALID_DF,
                                    x_col= 'filenames',
                                    y_col= 'labels',
                                    target_size= IMG_SIZE,
                                    class_mode= 'categorical',
                                    color_mode= 'rgb',
                                    shuffle= True,
                                    batch_size= BATCH_SIZE,
                                    seed=88
)
test_gen= gen2.flow_from_dataframe(test_df,
                                   x_col= 'filenames',
                                   y_col= 'labels',
                                   target_size= IMG_SIZE,
                                   class_mode= 'categorical',
                                   color_mode= 'rgb',
                                   shuffle= True,
                                   batch_size= 325,
                                   seed=88
)


STEPS= int( len(train_gen.labels)/BATCH_SIZE)
print(STEPS)

# import shutil
# shutil.rmtree('/content/Preparedata')

# Load the pre-trained ResNet50 model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dense
from tensorflow.keras.models import Model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the initial 5 layers
for layer in base_model.layers[:5]:
    layer.trainable = False

# Create a new model on top
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = BatchNormalization()(x)
#x = Dense(128, activation= 'relu',kernel_initializer='he_uniform')(x)
#x = BatchNormalization()(x)
#x = Dropout(0.3)(x)
predictions = Dense(4, activation= "softmax")(x)
model = Model(inputs=base_model.input, outputs=predictions)

from tensorflow.keras.optimizers import Adam
initial_learning_rate = 0.0001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=40,
    decay_rate=0.96,
    staircase=False)


model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-08), #optimizer=Adam(learning_rate=0.00001,decay = 10e-5),
              metrics=['accuracy'])
             # option = run_opts)

checkpoint_path = '/content/drive/MyDrive/checkpoints'
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)
history =         model.fit(x=train_gen,
                            epochs=1,
                            validation_data=valid_gen,
                            steps_per_epoch=None,
                            workers=2,
                            callbacks=[cp_callback]
                            )
model.save_weights(checkpoint_path)

model.save('My first project')


# model.save('Resnet50_Alzheimers.h5')
# model.compile(
#     loss='categorical_crossentropy',
#     optimizer=optimizers.RMSprop(learning_rate=1e-4),
#     metrics=['accuracy']
# )
# checkpoint_path = ''
# checkpoint_dir = os.path.dirname(checkpoint_path)

# # Create a callback that saves the model's weights
# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
#                                                  save_weights_only=True,
#                                                  verbose=1)
# # Train the model
# history= model.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // batch_size,
#     epochs=1,
#     validation_data=val_generator,
#     validation_steps=val_generator.samples,
#      callbacks=[cp_callback]
# )

# model.save_weights(checkpoint_path)

# model.save('Resnet50_Alzheimers.h5')